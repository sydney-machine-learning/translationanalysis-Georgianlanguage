
With 7,164 spoken or signed languages in use today[], only 23 of these account for more than half of the world's population[]. Consequently such languages command extensive online presence with large multilingual corpora featuring a rich variety of expert translations and annotations.

In contrast, some 7100+ remaining languages that lack the same level of availability are often referred to as Low Resource Languages. With nearly 500 world languages already extinct[], one language disappearing approximately every month[], and over 3,000 endangered languages with less than 1,000 active users each, majority of currently recognised languages may never get an opportunity to change this. Hence the growing interest in attempting to apply modern machine learning techniques to under resourced languages to aid with their translation, understanding, and exposure to the rest of the world.

Georgian language, a prominent member of the Katvelian family, featuring a very distinct alphabet and approximately 4 million active users, is an example of a Low Resource Language. Although it is classified as being institutionalised[] meaning that it has no risk of going extinct even if no native speakers remained - similar to the Latin language - it's online presence is fairly limited, in part due to its unique and intricate script with debated origins. Despite Georgia having a rich literary tradition with over 500,000 books in Georgian language being on display in The National Parliamentary Library of Georgia[], less than 500 works are known to be translated into a foreign language[]. 

Recent developments in machine learning techniques, powered largely by a combination of growing compute power and big data, have led to tremendous advancements in  recent years with regards to Natural Language Processing including but not limited to machine translation and semantic analysis. Deep Neural Networks models became immensely popular in late 2000's due to their robustness and flexibility, and started to outperform traditional statistical methods in various applications. Google Translate, a dominant machine translation tool at that time, made a switch to Deep Learning models in 2016 based on attention mechanisms developed a couple of years prior to that[]. Further research in this area along with an introduction of transformer architecture in 2017 and a consequent development of BERT language model the following year led to a state of the art machine translation tool at that time. BERT's hegemony remained largely unchallenged for a couple of years until a mainstream appearance of Large Language Models pioneered by OpenAI through a customer facing release of their now ubiquitous ChatGPT AI model in 2022. This has given a rapid rise in similar architectures spawned by the Big Tech in the months to come, namely Gemini, Cardano, and DeepL just to name a few. Regrettably DeepL, often hailed as the most refined and advanced machine translation tool on the market today is only available in 32 high resource languages at the time of this report. We will attempt to focus our interest 3 translation tools readily available to us  - ChatGPT 4.0, Google Gemini 1.5, and Google Translate.

In doing so, we will describe our methodology in section (2), present our  findings in section (3),  have a detailed discussion of the results in section (4),   and outline our conclusions in section (5).
